{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Zombie_Ducky_Object_Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ORTI9PU6heJa",
        "8sJzbL4iheJa"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanJoumaa/Zombie_Rubber_Ducky_Object_Detection/blob/main/Zombie_Ducky_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Zombie Ruber Ducky Detection\n",
        "\n",
        "We will use the Object Detection API and retrain [RetinaNet](https://arxiv.org/abs/1708.02002) to spot Zombies and Rubber Duckies using just 5 training images of each. We will setup the model to restore pretrained weights and fine tune the classification layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-J7whqrkIl"
      },
      "source": [
        "We will start by installing the Tensorflow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# uncomment the next line to delete an existing models directory\n",
        "!rm -rf ./models/\n",
        "\n",
        "# clone the Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# install the Object Detection API\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tUtyyVrkIt"
      },
      "source": [
        "## Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-auwvBQrkIw"
      },
      "source": [
        "### Import Object Detection API packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YihOFxxrkIw"
      },
      "source": [
        "# import the label map utility module\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "# import module for reading and updating configuration files.\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "# import module for visualization. use the alias `viz_utils`\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "# import module for building the detection model\n",
        "from object_detection.builders import model_builder\n",
        "### END CODE HERE ###\n",
        "\n",
        "# import module for utilities in Colab\n",
        "from object_detection.utils import colab_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilities for loading images and plotting detections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "    path: a file path.\n",
        "\n",
        "    Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    (im_width, im_height) = image.size\n",
        "    \n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "    \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "    Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "          and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "          this function assumes that the boxes to be plotted are groundtruth\n",
        "          boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "          category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_np_with_annotations = image_np.copy()\n",
        "    \n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        min_score_thresh=0.8)\n",
        "    \n",
        "    if image_name:\n",
        "        plt.imsave(image_name, image_np_with_annotations)\n",
        "    \n",
        "    else:\n",
        "        plt.imshow(image_np_with_annotations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "## Download the Zombie data\n",
        "\n",
        "We will get 5 images of zombies that we will use for training. \n",
        "- The zombies are hosted in a Google bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSFoJz2Cgzs"
      },
      "source": [
        "# download the images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training-zombie.zip \\\n",
        "    -O ./training-zombie.zip\n",
        "\n",
        "# unzip to a local directory\n",
        "local_zip = './training-zombie.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./training')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzSGUDsrkI7"
      },
      "source": [
        "### Get the Rubber Ducky Data & Visualize the training images\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQy3ND7EpFQM"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "# assign the name (string) of the directory containing the training images\n",
        "train_image_dir = './training'\n",
        "\n",
        "# Assign the zombie class ID\n",
        "zombie_class_id = 1\n",
        "duck_class_id = 2\n",
        "\n",
        "# declare an empty list\n",
        "train_images_np = []\n",
        "train_class_np=[]\n",
        "# run a for loop for each image\n",
        "for i in range(1, 6): \n",
        "\n",
        "    # define the path (string) for each image\n",
        "    image_path = os.path.join(train_image_dir,'training-zombie'+str(i)+'.jpg')\n",
        "    print(image_path)\n",
        "\n",
        "    # load images into numpy arrays and append to a list\n",
        "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "    train_class_np.append(np.array([zombie_class_id],dtype=np.int32))\n",
        "\n",
        "train_image_dir2 = 'models/research/object_detection/test_images/ducky/train/'\n",
        "for i in range(1, 6):\n",
        "  image_path = os.path.join(train_image_dir2, 'robertducky' + str(i) + '.jpg')\n",
        "  train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "  train_class_np.append(np.array([duck_class_id],dtype=np.int32))\n",
        "\n",
        "# configure plot settings via rcParams\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# plot images\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    plt.subplot(1, 11, idx+1)\n",
        "    plt.imshow(train_image_np)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "## Prepare data for training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqDVC6epheJZ"
      },
      "source": [
        "# Define the list of ground truth boxes\n",
        "gt_boxes = [np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "            np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "            np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "            np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "            np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "            np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n",
        "            np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n",
        "            np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n",
        "            np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n",
        "            np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxd66yq-M9Va"
      },
      "source": [
        "# print the coordinates of the ground truth boxes\n",
        "for gt_box in gt_boxes:\n",
        "  print(gt_box)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeDQaSdrkJC"
      },
      "source": [
        "\n",
        "\n",
        "### Define the category index dictionary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBqFVMcweF-"
      },
      "source": [
        "# Assign the zombie and rubber ducky class ID\n",
        "zombie_class_id = 1\n",
        "duck_class_id = 2\n",
        "# define a dictionary describing the zombie class\n",
        "category_index = {zombie_class_id : {'id'  : zombie_class_id, 'name': 'Zombie'},\n",
        "                  duck_class_id: {'id': duck_class_id, 'name': 'rubber_ducky'}}\n",
        "\n",
        "# Specify the number of classes that the model will predict\n",
        "num_classes = 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPy7vfCCrkJF"
      },
      "source": [
        "print(category_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CYKSo6s2Ehc"
      },
      "source": [
        "print(np.array([duck_class_id],dtype=np.int32)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvj7Q0oYk75"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "- Convert the class labels to one-hot representations\n",
        "- convert everything (i.e. train images, gt boxes and class labels) to tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68ot6BkrkJH"
      },
      "source": [
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
        "# automatically in our training binaries, but we need to reproduce it here.\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "\n",
        "# lists containing the one-hot encoded classes and ground truth boxes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np, tr_class_np) in zip(train_images_np, gt_boxes,train_class_np):\n",
        "    \n",
        "    # convert training image to tensor, add batch dimension, and add to list\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "    \n",
        "    # convert numpy array to tensor, then add to list\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "    \n",
        "    # apply offset to to have zero-indexed ground truth classes\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(tr_class_np - label_id_offset)\n",
        "    print(zero_indexed_groundtruth_classes)\n",
        "    # do one-hot encoding to ground truth classes\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_groundtruth_classes, num_classes))\n",
        "print(gt_classes_one_hot_tensors)\n",
        "print('Done prepping data.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "## Visualize the zombies and rubber duckies with their ground truth bounding boxes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "source": [
        "# give boxes a score of 100%\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "\n",
        "# define the figure size\n",
        "plt.figure(figsize=(30, 30))\n",
        "\n",
        "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
        "for idx in range(8):\n",
        "    plt.subplot(3, 3, idx+1)\n",
        "    plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      train_class_np[idx],\n",
        "      dummy_scores, category_index)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "## Download the checkpoint containing the pre-trained weights\n",
        "\n",
        "  - Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.\n",
        "  - Untar (decompress) the tar file\n",
        "  - Move the decompressed checkpoint to `models/research/object_detection/test_data/`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J16r3NChD-7"
      },
      "source": [
        "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "    \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "# copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG_YT7UrkJQ"
      },
      "source": [
        "## Configure the model\n",
        "Here, we will configure the model for our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eHyNmxheJa"
      },
      "source": [
        "\n",
        "\n",
        "### Locate and read from the configuration file\n",
        "\n",
        "#### pipeline_config\n",
        "- In the Colab, on the left side table of contents, click on the folder icon to display the file browser for the current workspace.  \n",
        "- Navigate to `models/research/object_detection/configs/tf2`.  The folder has multiple .config files.  \n",
        "- Look for the file corresponding to ssd resnet 50 version 1 640x640.\n",
        "- We can double-click the config file to view its contents. \n",
        "- Set the `pipeline_config` to a string that contains the full path to the resnet config file, in other words: `models/research/.../... .config`\n",
        "\n",
        "\n",
        "#### configs\n",
        "If we look at the module [config_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py) that we imported, it contains the following function:\n",
        "\n",
        "```\n",
        "def get_configs_from_pipeline_file(pipeline_config_path, config_override=None):\n",
        "```\n",
        "- Use this function to load the configuration from the `pipeline_config`.\n",
        "  - `configs` will now contain a dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59sEM6wXheJa"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
        "pipeline_config = '/content/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "\n",
        "# Load the configuration file into a dictionary\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "\n",
        "\n",
        "# See what configs looks like\n",
        "configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3AzqBvheJa"
      },
      "source": [
        "# Read in the object stored at the key 'model' of the configs dictionary\n",
        "model_config = configs.get('model')\n",
        "\n",
        "# see what model_config looks like\n",
        "model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxHqQK1eheJa"
      },
      "source": [
        "### Modify model_config\n",
        "- Modify num_classes from the default `90` to the `num_classes` that we set earlier in this notebook.\n",
        "  \n",
        "- Freeze batch normalization \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyT4BUbaMeG-"
      },
      "source": [
        "# Modify the number of classes from its default of 90\n",
        "model_config.ssd.num_classes = num_classes\n",
        "\n",
        "# Freeze batch normalization\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "\n",
        "# See what model_config now looks like after we've customized it!\n",
        "model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnuT2rfheJa"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoRTeV_rkJT"
      },
      "source": [
        "detection_model = model_builder.build(model_config=model_config, is_training=True)\n",
        "\n",
        "print(type(detection_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C97Zo0OJnOf"
      },
      "source": [
        "## Restore weights from our checkpoint\n",
        "\n",
        "Now, we will selectively restore weights from our checkpoint.\n",
        "- Our end goal is to create a custom model which reuses parts of, but not all of the layers of RetinaNet (currently stored in the variable `detection_model`.)\n",
        "  - The parts of RetinaNet that we want to reuse are:\n",
        "    - Feature extraction layers\n",
        "    - Bounding box regression prediction layer\n",
        "  - The part of RetinaNet that we will not want to reuse is the classification prediction layer (since we will define and train our own classification layer specific to zombies).\n",
        "  - For the parts of RetinaNet that we want to reuse, we will also restore the weights from the checkpoint that we selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BK06DyFheJa"
      },
      "source": [
        "\n",
        "### Define Checkpoints for the box predictor\n",
        "\n",
        "- Define `box_predictor_checkpoint` to be checkpoint for these two layers of the `detection_model`'s box predictor:\n",
        "  - The base tower layer (the layers the precede both the class prediction and bounding box prediction layers).\n",
        "  - The box prediction head (the prediction layer for bounding boxes).\n",
        "- Note that we won't include the class prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPWWYbyrkJY"
      },
      "source": [
        "tmp_box_predictor_checkpoint = tf.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    _box_prediction_head = detection_model._box_predictor._box_prediction_head\n",
        ")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlefNC_XheJa"
      },
      "source": [
        "# Check the datatype of this checkpoint\n",
        "type(tmp_box_predictor_checkpoint)\n",
        "\n",
        "#Output should be: tensorflow.python.training.tracking.util.Checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9X0BfufheJa"
      },
      "source": [
        "# Check the variables of this checkpoint\n",
        "vars(tmp_box_predictor_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORTI9PU6heJa"
      },
      "source": [
        "\n",
        "We should expect to see a list of variables that include the following:\n",
        "```\n",
        "'_base_tower_layers_for_heads': DictWrapper({'box_encodings': ListWrapper([]), 'class_predictions_with_background': ListWrapper([])}),\n",
        "'_box_prediction_head': <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7fefac014710>,\n",
        " ... \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqMyPTU1rkJa"
      },
      "source": [
        "### Define the temporary model checkpoint\n",
        "\n",
        "Now define `tmp_model_checkpoint` so that it points to these two layers:\n",
        "- The feature extractor of the detection model.\n",
        "- The temporary box predictor checkpoint that we just defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqGfrawrkJa"
      },
      "source": [
        "tmp_model_checkpoint = tf.train.Checkpoint(\n",
        "    _box_predictor = tmp_box_predictor_checkpoint,\n",
        "    _feature_extractor = detection_model._feature_extractor\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXZBgRtheJa"
      },
      "source": [
        "# Check the datatype of this checkpoint\n",
        "type(tmp_model_checkpoint)\n",
        "\n",
        "\n",
        "# Output should be: tensorflow.python.training.tracking.util.Checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSxZqzU4heJa"
      },
      "source": [
        "# Check the vars of this checkpoint\n",
        "vars(tmp_model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJzbL4iheJa"
      },
      "source": [
        "Among the variables of this checkpoint, we should see:\n",
        "```\n",
        "'_box_predictor': <tensorflow.python.training.tracking.util.Checkpoint at 0x7fefac044a20>,\n",
        " '_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7fefac0240b8>,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_BtL4ZZKVvV"
      },
      "source": [
        "\n",
        "### Restore the checkpoint\n",
        "\n",
        "We can now restore the checkpoint.\n",
        "\n",
        "First, find and set the `checkpoint_path`\n",
        "\n",
        "- checkpoint_path: \n",
        "  - Using the \"files\" browser in the left side of Colab, navigate to `models -> research -> object_detection -> test_data`. \n",
        "  - If we completed the previous code cell that downloads and moves the checkpoint, we will see a subfolder named \"checkpoint\".  \n",
        "    - The 'checkpoint' folder contains three files:\n",
        "      - checkpoint\n",
        "      - ckpt-0.data-00000-of-00001\n",
        "      - ckpt-0.index\n",
        "    - Set checkpoint_path to the path to the full path `models/.../ckpt-0` \n",
        "      - Notice that we don't want to include a file extension after `ckpt-0`. \n",
        "      - If we do set it to `ckpt-0.index`, there won't be any immediate error message, but later during training, we will notice that our model's loss doesn't improve, which means that the pre-trained weights were not restored properly.\n",
        "\n",
        "Next, define one last checkpoint using `tf.train.Checkpoint()`.\n",
        "- For the single keyword argument, \n",
        "  - Set the key as `model=` \n",
        "  - Set the value to our temporary model checkpoint that we just defined.\n",
        "- **IMPORTANT**: We will need to set the keyword argument as `model=` and not something else like `detection_model=`.\n",
        "- If we set this keyword argument to anything else, it won't show an immmediate error, but when we train our model on the zombie and rubber ducky images, our model loss will not decrease.\n",
        "\n",
        "Finally, call this checkpoint's `.restore()` function, passing in the path to the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkvDUUarkJg"
      },
      "source": [
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    model=tmp_model_checkpoint\n",
        ")\n",
        "\n",
        "# Restore the checkpoint to the checkpoint path\n",
        "checkpoint.restore(checkpoint_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6oFoxTrkJi"
      },
      "source": [
        "### Run a dummy image to generate the model variables\n",
        "\n",
        "Run a dummy image through the model so that variables are created. We need to select the trainable variables later. Try running `len(detection_model.trainable_variables)` in a code cell and we will get `0`. We will pass in a dummy image through the forward pass to create these variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MANTSP9LrkJi"
      },
      "source": [
        "# use the detection model's `preprocess()` method and pass a dummy image\n",
        "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "\n",
        "# run a prediction with the preprocessed image and shapes\n",
        "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
        "\n",
        "# postprocess the predictions into final detections\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0KvPP-krkJn"
      },
      "source": [
        "assert len(detection_model.trainable_variables) > 0, \"Pass in a dummy image to create the trainable variables.\"\n",
        "\n",
        "print(detection_model.weights[0].shape)\n",
        "print(detection_model.weights[231].shape)\n",
        "print(detection_model.weights[462].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "## Eager mode custom training loop\n",
        "\n",
        "With the data and model now setup, we can now proceed to configure the training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlV4PoPrkJq"
      },
      "source": [
        "### Set training hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHoF4mUrv5-"
      },
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# set the batch_size\n",
        "batch_size = 10\n",
        "\n",
        "# set the number of batches\n",
        "num_batches = 100\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# set the optimizer and pass in the learning_rate\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MaJmHmrkJs"
      },
      "source": [
        "## Choose the layers to fine-tune\n",
        "\n",
        "To make use of transfer learning and pre-trained weights, we will train just certain parts of the detection model, namely, the last prediction layers.\n",
        "- We will inspect the layers of `detection_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91r2dk-7heJb"
      },
      "source": [
        "# Inspect the layers of detection_model\n",
        "for i,v in enumerate(detection_model.trainable_variables):\n",
        "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wtU67JZheJb"
      },
      "source": [
        "Notice that there are some layers whose names are prefixed with the following:\n",
        "```\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/BoxPredictionTower\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/ClassPredictionTower\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "- Recall that when inspecting the source code to restore the checkpoints ([convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py)) we noticed that:\n",
        "  - `_base_tower_layers_for_heads`: refers to the layers that are placed right before the prediction layer\n",
        "  - `_box_prediction_head` refers to the prediction layer for the bounding boxes\n",
        "  - `_prediction_heads`: refers to the set of prediction layers (both for classification and for bounding boxes)\n",
        "\n",
        "\n",
        "So we can see that in the source code for this model, \"tower\" refers to layers that are before the prediction layer, and \"head\" refers to the prediction layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6S9QjSWheJb"
      },
      "source": [
        "### Select the prediction layer variables\n",
        "\n",
        "Based on inspecting the `detection_model.trainable_variables`, we will select the prediction layer variables that we will fine tune:\n",
        "- The bounding box head variables (which predict bounding box coordinates)\n",
        "- The class head variables (which predict the class/category)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDdvGRCYrkJt"
      },
      "source": [
        "# define a list that contains the layers that we wish to fine tune\n",
        "to_fine_tune = []\n",
        "for v in detection_model.trainable_variables:\n",
        "  if v.name.startswith('WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutional'):\n",
        "    to_fine_tune.append(v)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwzzGkzyrkJv"
      },
      "source": [
        "print(to_fine_tune[0].name)\n",
        "print(to_fine_tune[2].name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-VK-GB2rkJy"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```txt\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvhZZu2rkJy"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We will define a function that handles training for one batch, which we will later use in our training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUKsYPLheJb"
      },
      "source": [
        "### Define the training step\n",
        "We will define the function below to set up one training step.\n",
        "- Preprocess the images\n",
        "- Make a prediction\n",
        "- Calculate the loss (and make sure the loss function has the ground truth to compare with the prediction)\n",
        "- Calculate the total loss:\n",
        "  - `total_loss` = `localization_loss + classification_loss`\n",
        "- Calculate gradients with respect to the variables we selected to train.\n",
        "- Optimize the model's variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "nttIf_ZgheJb"
      },
      "source": [
        "@tf.function\n",
        "def train_step_fn(image_list,\n",
        "                groundtruth_boxes_list,\n",
        "                groundtruth_classes_list,\n",
        "                model,\n",
        "                optimizer,\n",
        "                vars_to_fine_tune):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Provide the ground truth to the model\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Preprocess the images\n",
        "        preprocessed_image_list = []\n",
        "        true_shape_list = []\n",
        "\n",
        "        for img in image_list:\n",
        "            processed_img, true_shape = model.preprocess(img)\n",
        "            preprocessed_image_list.append(processed_img)\n",
        "            true_shape_list.append(true_shape)\n",
        "\n",
        "        preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "        # Calculate the total loss (sum of both losses)\n",
        "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
        "        \n",
        "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "\n",
        "        # Calculate the gradients\n",
        "        gradients = tape.gradient([total_loss], vars_to_fine_tune)\n",
        "\n",
        "        # Optimize the model's selected variables\n",
        "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n7imaJRrkJ1"
      },
      "source": [
        "## Run the training loop\n",
        "\n",
        "Run the training loop using the training step function that we just defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgpDQ3JbrkJ3"
      },
      "source": [
        "print('Start fine-tuning!', flush=True)\n",
        "\n",
        "for idx in range(num_batches):\n",
        "    # Grab keys for a random subset of examples\n",
        "    all_keys = list(range(len(train_images_np)))\n",
        "    random.shuffle(all_keys)\n",
        "    example_keys = all_keys[:batch_size]\n",
        "\n",
        "    # Get the ground truth\n",
        "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "    \n",
        "    # get the images\n",
        "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "    # Training step (forward pass + backwards pass)\n",
        "    total_loss = train_step_fn(image_tensors, \n",
        "                               gt_boxes_list, \n",
        "                               gt_classes_list,\n",
        "                               detection_model,\n",
        "                               optimizer,\n",
        "                               to_fine_tune)\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "        + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "## Load test images and run inference with new model!\n",
        "\n",
        "We can now test our model on a new set of images. The cell below downloads 237 images of a walking zombie and stores them in a `results/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL697_MnHcJF"
      },
      "source": [
        "# uncomment to delete existing files\n",
        "!rm zombie-walk-frames.zip\n",
        "!rm -rf ./zombie-walk\n",
        "!rm -rf ./results\n",
        "\n",
        "# download test images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/zombie-walk-frames.zip \\\n",
        "    -O zombie-walk-frames.zip\n",
        "\n",
        "# unzip test images\n",
        "local_zip = './zombie-walk-frames.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./results')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LJJ8wWhzlZ"
      },
      "source": [
        "We will load these images into numpy arrays to prepare it for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcE6OwrHQJya"
      },
      "source": [
        "test_image_dir = './results/'\n",
        "test_images_np = []\n",
        "\n",
        "# load images into a numpy array. this will take a few minutes to complete.\n",
        "for i in range(0, 237):\n",
        "    image_path = os.path.join(test_image_dir, 'zombie-walk' + \"{0:04}\".format(i) + '.jpg')\n",
        "    print(image_path)\n",
        "    test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GIbqkzrkJ_"
      },
      "source": [
        "\n",
        "\n",
        "### Preprocess, predict, and post process an image\n",
        "\n",
        "We will define a function that returns the detection boxes, classes, and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aXt-bodrkKA"
      },
      "source": [
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "    Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    \n",
        "    # use the detection model's postprocess() method to get the the final detections\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    \n",
        "    return detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rY0aDJaFKW2"
      },
      "source": [
        "### Test on Ruber Ducky and Zombie Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlOoOSAsHfrL"
      },
      "source": [
        "#### Test on Rubber Ducky Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f5X0SYN_g-X"
      },
      "source": [
        "test_image_dir = 'models/research/object_detection/test_images/ducky/test/'\r\n",
        "test_images_np = []\r\n",
        "for i in range(1, 50):\r\n",
        "  image_path = os.path.join(test_image_dir, 'out' + str(i) + '.jpg')\r\n",
        "  test_images_np.append(np.expand_dims(\r\n",
        "      load_image_into_numpy_array(image_path), axis=0))\r\n",
        "\r\n",
        "\r\n",
        "label_id_offset = 1\r\n",
        "for i in range(len(test_images_np)):\r\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\r\n",
        "  detections = detect(input_tensor)\r\n",
        "\r\n",
        "  plot_detections(\r\n",
        "      test_images_np[i][0],\r\n",
        "      detections['detection_boxes'][0].numpy(),\r\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)\r\n",
        "      + label_id_offset,\r\n",
        "      detections['detection_scores'][0].numpy(),\r\n",
        "      category_index, figsize=(15, 20), image_name=\"Rubber_Ducky_\" + ('%02d' % i) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj6SZBluHmLJ"
      },
      "source": [
        "#### Test on Zombie Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRZH-P3bHlQg"
      },
      "source": [
        "test_image_dir = './results/'\r\n",
        "test_images_np = []\r\n",
        "\r\n",
        "# load images into a numpy array. this will take a few minutes to complete.\r\n",
        "for i in range(0, 237):\r\n",
        "    image_path = os.path.join(test_image_dir, 'zombie-walk' + \"{0:04}\".format(i) + '.jpg')\r\n",
        "    print(image_path)\r\n",
        "    test_images_np.append(np.expand_dims(\r\n",
        "      load_image_into_numpy_array(image_path), axis=0))\r\n",
        "label_id_offset = 1\r\n",
        "for i in range(len(test_images_np)):\r\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\r\n",
        "  detections = detect(input_tensor)\r\n",
        "\r\n",
        "  plot_detections(\r\n",
        "      test_images_np[i][0],\r\n",
        "      detections['detection_boxes'][0].numpy(),\r\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)\r\n",
        "      + label_id_offset,\r\n",
        "      detections['detection_scores'][0].numpy(),\r\n",
        "      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZe426eNHFQg"
      },
      "source": [
        "### Test on an uploaded image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqRwtlshCGBP"
      },
      "source": [
        "image_path = '/content/2_ducky.jpg' ##Path of uploaded image\r\n",
        "image_np = load_image_into_numpy_array(image_path)\r\n",
        "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\r\n",
        "\r\n",
        "detections = detect(input_tensor)\r\n",
        "label_id_offset = 1\r\n",
        "image_np_with_detections = image_np.copy()\r\n",
        "\r\n",
        "plot_detections(\r\n",
        "      image_np_with_detections,\r\n",
        "      detections['detection_boxes'][0].numpy(),\r\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)+ label_id_offset,\r\n",
        "      detections['detection_scores'][0].numpy(),\r\n",
        "      category_index, figsize=(15, 20))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}